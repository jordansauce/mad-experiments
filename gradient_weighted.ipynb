{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jordan/miniconda3/envs/MAD/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from contextlib import contextmanager\n",
    "from pathlib import Path\n",
    "import functools\n",
    "\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange, reduce, einsum\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import torch\n",
    "from cupbearer import detectors, tasks, utils, scripts\n",
    "from torch import Tensor, nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model attn-only-1l into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/nadro/Documents/AI_safety/CHAI/MAD_code/cupbearer/src/cupbearer/tasks/tiny_natural_mechanisms.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(cache_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "task = tasks.tiny_natural_mechanisms(\"hex\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".cupbearer_cache/tiny_natural_mechanisms/hex_task.json\") as f:\n",
    "    task_config = json.load(f)\n",
    "\n",
    "# cpu to avoid MPS issues with tensors that are too large\n",
    "effect_tokens = torch.tensor(\n",
    "    task_config[\"effect_tokens\"], dtype=torch.long, device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "embed\n",
      "hook_embed\n",
      "pos_embed\n",
      "hook_pos_embed\n",
      "blocks\n",
      "blocks.0\n",
      "blocks.0.ln1\n",
      "blocks.0.ln1.hook_scale\n",
      "blocks.0.ln1.hook_normalized\n",
      "blocks.0.attn\n",
      "blocks.0.attn.hook_k\n",
      "blocks.0.attn.hook_q\n",
      "blocks.0.attn.hook_v\n",
      "blocks.0.attn.hook_z\n",
      "blocks.0.attn.hook_attn_scores\n",
      "blocks.0.attn.hook_pattern\n",
      "blocks.0.attn.hook_result\n",
      "blocks.0.hook_attn_in\n",
      "blocks.0.hook_q_input\n",
      "blocks.0.hook_k_input\n",
      "blocks.0.hook_v_input\n",
      "blocks.0.hook_mlp_in\n",
      "blocks.0.hook_attn_out\n",
      "blocks.0.hook_mlp_out\n",
      "blocks.0.hook_resid_pre\n",
      "blocks.0.hook_resid_post\n",
      "ln_final\n",
      "ln_final.hook_scale\n",
      "ln_final.hook_normalized\n",
      "unembed\n"
     ]
    }
   ],
   "source": [
    "for name, _ in task.model.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effect_prob_func(logits):\n",
    "    assert logits.ndim == 3\n",
    "    probs = logits.softmax(-1)\n",
    "    # Sum over vocab (but not batch) dim:\n",
    "    return probs[:, -1, effect_tokens].sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientWeightedDetector(detectors.AnomalyDetector):\n",
    "    def __init__(self, names: list[str], output_func_for_grads):\n",
    "        super().__init__()\n",
    "        self.names = names\n",
    "        self.output_func_for_grads = output_func_for_grads\n",
    "\n",
    "    def _train(\n",
    "        self,\n",
    "        trusted_data: torch.utils.data.Dataset,\n",
    "        untrusted_data: torch.utils.data.Dataset | None,\n",
    "        save_path: Path | str | None,\n",
    "        batch_size: int = 64,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        assert trusted_data is not None\n",
    "\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "            trusted_data, batch_size=batch_size, shuffle=False\n",
    "        )\n",
    "        example_batch = next(iter(data_loader))\n",
    "        example_inputs = utils.inputs_from_batch(example_batch)\n",
    "        example_activations = utils.get_activations(\n",
    "            self.model, self.names, example_inputs\n",
    "        )\n",
    "\n",
    "        # v is an entire batch, v[0] are activations for a single input\n",
    "        activation_sizes = {k: v[0].size() for k, v in example_activations.items()}\n",
    "        device = next(iter(example_activations.values())).device\n",
    "\n",
    "        for k, size in activation_sizes.items():\n",
    "            if len(size) not in (1, 2):\n",
    "                raise ValueError(\n",
    "                    f\"Activation size {size} of {k} is not supported. \"\n",
    "                    \"Activations must be either 1D or 2D (in which case separate \"\n",
    "                    \"covariance matrices are learned along the first dimension).\"\n",
    "                )\n",
    "        self.means = {\n",
    "            k: torch.zeros(size[-1], device=device)\n",
    "            for k, size in activation_sizes.items()\n",
    "        }\n",
    "        self.Pi = {\n",
    "            k: torch.zeros((size[-1], size[-1]), device=device)\n",
    "            for k, size in activation_sizes.items()\n",
    "        }\n",
    "        self._n = 0\n",
    "\n",
    "        for i, batch in enumerate(tqdm(data_loader)):\n",
    "            inputs = utils.inputs_from_batch(batch).to(device)\n",
    "            activations, grads = utils.get_activations_and_grads(\n",
    "                self.model, self.names, self.output_func_for_grads, inputs\n",
    "            )\n",
    "\n",
    "            for k in self.names:\n",
    "                # Flatten the activations to (batch, dim)\n",
    "                activation = activations[k]\n",
    "                grad = grads[k]\n",
    "\n",
    "                assert activation.shape == grad.shape\n",
    "\n",
    "                if activation.ndim == 3:\n",
    "                    activation = rearrange(\n",
    "                        activation, \"batch independent dim -> (batch independent) dim\"\n",
    "                    )\n",
    "                    grad = rearrange(\n",
    "                        grad, \"batch independent dim -> (batch independent) dim\"\n",
    "                    )\n",
    "                assert activation.ndim == 2, activation.shape\n",
    "\n",
    "                new_n = len(activation)\n",
    "                total_n = self._n + new_n\n",
    "\n",
    "                new_mean = activation.mean(dim=0)\n",
    "                self.means[k] = (self._n * self.means[k] + new_n * new_mean) / total_n\n",
    "\n",
    "                # Compute outer product, then take the mean over the batch dimension\n",
    "                new_C = torch.einsum(\"bi, bj -> ij\", grad, grad) / new_n\n",
    "                self.Pi[k] = (self.Pi[k] * self._n + new_C * new_n) / total_n\n",
    "\n",
    "                self._n = total_n\n",
    "\n",
    "    def _compute_layerwise_scores(self, batch):\n",
    "        inputs = utils.inputs_from_batch(batch).to(\"mps\")\n",
    "\n",
    "        activations, grads = utils.get_activations_and_grads(\n",
    "            self.model, self.names, self.output_func_for_grads, inputs\n",
    "        )\n",
    "        batch_size = next(iter(activations.values())).shape[0]\n",
    "\n",
    "        distances: dict[str, torch.Tensor] = {}\n",
    "        # Reshape activations to (batch, dim) for computing distances\n",
    "        for k in self.names:\n",
    "            activation = activations[k]\n",
    "            grad = grads[k]\n",
    "            assert activation.shape == grad.shape\n",
    "            if activation.ndim == 3:\n",
    "                activation = rearrange(\n",
    "                    activation, \"batch independent dim -> (batch independent) dim\"\n",
    "                )\n",
    "                grad = rearrange(\n",
    "                    grad, \"batch independent dim -> (batch independent) dim\"\n",
    "                )\n",
    "            assert activation.ndim == 2, activation.shape\n",
    "\n",
    "            delta = activation - self.means[k]\n",
    "            distance = torch.einsum(\"bi,ij,bj->b\", delta, self.Pi[k], delta)\n",
    "            distances[k] = distance\n",
    "\n",
    "        for k, v in distances.items():\n",
    "            # Unflatten distances so we can take the mean over the independent axis\n",
    "            distances[k] = rearrange(\n",
    "                v, \"(batch independent) -> batch independent\", batch=batch_size\n",
    "            ).mean(dim=1)\n",
    "\n",
    "        return distances\n",
    "\n",
    "    def _get_trained_variables(self, saving: bool = False):\n",
    "        return {\n",
    "            \"means\": self.means,\n",
    "            \"Pi\": self.Pi,\n",
    "        }\n",
    "\n",
    "    def _set_trained_variables(self, variables):\n",
    "        self.means = variables[\"means\"]\n",
    "        self.Pi = variables[\"Pi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cupbearer.tasks.tiny_natural_mechanisms.TinyNaturalMechanismsDataset at 0x7fb86a987e20>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.trusted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "trusted_data and untrusted_data must be None when passing a task",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.0.hook_attn_out.output\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# \"blocks.0.attn.hook_attn_scores.output\",\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# \"blocks.0.attn.hook_q.output\",\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mln_final.hook_normalized.output\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m ]\n\u001b[1;32m      7\u001b[0m detector \u001b[38;5;241m=\u001b[39m GradientWeightedDetector(names\u001b[38;5;241m=\u001b[39mnames, output_func_for_grads\u001b[38;5;241m=\u001b[39meffect_prob_func)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrusted_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrusted_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muntrusted_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntrusted_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/nadro/Documents/AI_safety/CHAI/MAD_code/cupbearer/src/cupbearer/detectors/anomaly_detector.py:190\u001b[0m, in \u001b[0;36mAnomalyDetector.train\u001b[0;34m(self, task, trusted_data, untrusted_data, model, batch_size, shuffle, num_workers, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel must be None when passing a task\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m--> 190\u001b[0m         trusted_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m untrusted_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    191\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrusted_data and untrusted_data must be None when passing a task\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m     trusted_data \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtrusted_data\n\u001b[1;32m    193\u001b[0m     untrusted_data \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39muntrusted_train_data\n",
      "\u001b[0;31mAssertionError\u001b[0m: trusted_data and untrusted_data must be None when passing a task"
     ]
    }
   ],
   "source": [
    "names = [\n",
    "    \"blocks.0.hook_attn_out.output\",\n",
    "    # \"blocks.0.attn.hook_attn_scores.output\",\n",
    "    # \"blocks.0.attn.hook_q.output\",\n",
    "    \"ln_final.hook_normalized.output\",\n",
    "]\n",
    "detector = GradientWeightedDetector(names=names, output_func_for_grads=effect_prob_func)\n",
    "detector.train(task, trusted_data=None, untrusted_data=None, save_path=None, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m px\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mdetector\u001b[49m\u001b[38;5;241m.\u001b[39mPi[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.0.hook_attn_out.output\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu(), binary_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'detector' is not defined"
     ]
    }
   ],
   "source": [
    "px.imshow(detector.Pi[\"blocks.0.hook_attn_out.output\"].cpu(), binary_string=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
